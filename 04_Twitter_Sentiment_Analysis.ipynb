{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tweet sentiment analysis\n",
    "\n",
    "In this section we will see how to extract features from tweets and use a classifier to classify the tweet as positive or negative.\n",
    "\n",
    "We will use a pandas DataFrames (http://pandas.pydata.org/) to store tweets and process them.\n",
    "Pandas DataFrames are very powerful python data-structures, like excel spreadsheets with the power of python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a DataFrame with each tweet using pandas\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getTweetID(tweet):\n",
    "    \"\"\" If properly included, get the ID of the tweet \"\"\"\n",
    "    return tweet.get('id')\n",
    "    \n",
    "def getUserIDandScreenName(tweet):\n",
    "    \"\"\" If properly included, get the tweet \n",
    "        user ID and Screen Name \"\"\"\n",
    "    user = tweet.get('user')\n",
    "    if user is not None:\n",
    "        uid = user.get('id')\n",
    "        screen_name = user.get('screen_name')\n",
    "        return uid, screen_name\n",
    "    else:\n",
    "        return (None, None)\n",
    "    \n",
    "\n",
    "    \n",
    "filename = 'AI2.txt'\n",
    "\n",
    "# create a list of dictionaries with the data that interests us\n",
    "tweet_data_list = []\n",
    "with open(filename, 'r') as fopen:\n",
    "    # each line correspond to a tweet\n",
    "    for line in fopen:\n",
    "        if line != '\\n':\n",
    "            tweet = json.loads(line.strip('\\n'))\n",
    "            tweet_id = getTweetID(tweet)\n",
    "            user_id = getUserIDandScreenName(tweet)[0]\n",
    "            text = tweet.get('text')\n",
    "            if tweet_id is not None:\n",
    "                tweet_data_list.append({'tweet_id' : tweet_id,\n",
    "                           'user_id' : user_id,\n",
    "                           'text' : text})\n",
    "\n",
    "# put everything in a dataframe\n",
    "tweet_df = pd.DataFrame.from_dict(tweet_data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5012, 3)\n",
      "Index(['text', 'tweet_id', 'user_id'], dtype='object')\n",
      "0                                      'This Isn't AI'\n",
      "1    RT @IoTRecruiting: How will Cognitive Computin...\n",
      "2                        Trou* https://t.co/FlQdwMFbmh\n",
      "3    RT @InvestorIdeas: https://t.co/ZFdyj2RNXV - #...\n",
      "4    DeepStack: Expert-level artificial intelligenc...\n",
      "Name: text, dtype: object\n",
      "0                                      'This Isn't AI'\n",
      "1    RT @IoTRecruiting: How will Cognitive Computin...\n",
      "2                        Trou* https://t.co/FlQdwMFbmh\n",
      "3    RT @InvestorIdeas: https://t.co/ZFdyj2RNXV - #...\n",
      "4    DeepStack: Expert-level artificial intelligenc...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweet_df.shape)\n",
    "print(tweet_df.columns)\n",
    "\n",
    "#print 5 first element of one of the column\n",
    "print(tweet_df.text.iloc[:5])\n",
    "# or\n",
    "print(tweet_df['text'].iloc[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'This Isn't AI'</td>\n",
       "      <td>860217132763754497</td>\n",
       "      <td>211638860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @IoTRecruiting: How will Cognitive Computin...</td>\n",
       "      <td>860217137058635776</td>\n",
       "      <td>3226000831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trou* https://t.co/FlQdwMFbmh</td>\n",
       "      <td>860217138908475397</td>\n",
       "      <td>3070524046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @InvestorIdeas: https://t.co/ZFdyj2RNXV - #...</td>\n",
       "      <td>860217141827518464</td>\n",
       "      <td>837339093214318593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepStack: Expert-level artificial intelligenc...</td>\n",
       "      <td>860217143756857344</td>\n",
       "      <td>766378262259986437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @IoTRecruiting: How will Cognitive Computin...</td>\n",
       "      <td>860217149192785920</td>\n",
       "      <td>2390579695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @IoTRecruiting: Honored to be ranked Top In...</td>\n",
       "      <td>860217150237270022</td>\n",
       "      <td>860148473873739779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I want Barnier to hand over the cheque with a ...</td>\n",
       "      <td>860217155568242693</td>\n",
       "      <td>740545840838774784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Iraqi refugee 'used asylum seekers to stage bu...</td>\n",
       "      <td>860217170948612096</td>\n",
       "      <td>14101483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @cabroncita: $BKD $BKDCD / $BKD.V Artificia...</td>\n",
       "      <td>860217174077706242</td>\n",
       "      <td>835220550209396736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            tweet_id  \\\n",
       "0                                    'This Isn't AI'  860217132763754497   \n",
       "1  RT @IoTRecruiting: How will Cognitive Computin...  860217137058635776   \n",
       "2                      Trou* https://t.co/FlQdwMFbmh  860217138908475397   \n",
       "3  RT @InvestorIdeas: https://t.co/ZFdyj2RNXV - #...  860217141827518464   \n",
       "4  DeepStack: Expert-level artificial intelligenc...  860217143756857344   \n",
       "5  RT @IoTRecruiting: How will Cognitive Computin...  860217149192785920   \n",
       "6  RT @IoTRecruiting: Honored to be ranked Top In...  860217150237270022   \n",
       "7  I want Barnier to hand over the cheque with a ...  860217155568242693   \n",
       "8  Iraqi refugee 'used asylum seekers to stage bu...  860217170948612096   \n",
       "9  RT @cabroncita: $BKD $BKDCD / $BKD.V Artificia...  860217174077706242   \n",
       "\n",
       "              user_id  \n",
       "0           211638860  \n",
       "1          3226000831  \n",
       "2          3070524046  \n",
       "3  837339093214318593  \n",
       "4  766378262259986437  \n",
       "5          2390579695  \n",
       "6  860148473873739779  \n",
       "7  740545840838774784  \n",
       "8            14101483  \n",
       "9  835220550209396736  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the first 10 rows\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from the tweets\n",
    "\n",
    "#### 1) Tokenize the tweet in a list of words\n",
    "\n",
    "This part uses concepts from [Naltural Langage Processing](https://en.wikipedia.org/wiki/Natural_language_processing).\n",
    "We will use a tweet tokenizer I built based on TweetTokenizer from NLTK (http://www.nltk.org/).\n",
    "You can see how it works by opening the file TwSentiment.py. The goal is to process any tweets and extract a list of words taking into account usernames, hashtags, urls, emoticons and all the informal text we can find in tweets. We also want to reduce the number of features by doing some transformations such as putting all the words in lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TwSentiment import CustomTweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CustomTweetTokenizer(preserve_case=False, # keep Upper cases\n",
    "                                 reduce_len=True, # reduce repetition of letter to a maximum of three\n",
    "                                 strip_handles=False, # remove usernames (@mentions)\n",
    "                                 normalize_usernames=True, # replace all mentions to \"@USER\"\n",
    "                                 normalize_urls=True, # replace all urls to \"URL\"\n",
    "                                 keep_allupper=True) # keep upercase for words that are all in uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'This Isn't AI'\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "tweet_df.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\", 'this', \"isn't\", 'AI', \"'\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(tweet_df.text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', '!', 'this', 'is', 'SO', 'coool', '!', ':)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# other examples\n",
    "tokenizer.tokenize('Hey! This is SO cooooooooooooooooool! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', '!', 'this', 'is', 'so', 'coool', '!', ':)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('Hey! This is so cooooooool! :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Define the features that will represent the tweet\n",
    "We will use the occurrence of words and pair of words (bigrams) as features.\n",
    "\n",
    "This corresponds to a bag-of-words representation (https://en.wikipedia.org/wiki/Bag-of-words_model): we just count each words (or [n-grams](https://en.wikipedia.org/wiki/N-gram)) without taking account their order. For document classification, the frequency of occurence of each words is usually taken as a feature. In the case of tweets, they are so short that we can just count each words once.\n",
    "\n",
    "Using pair of words allows to capture some of the context in which each words appear. This helps capturing the correct meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\": True,\n",
       " 'this': True,\n",
       " \"isn't\": True,\n",
       " 'AI': True,\n",
       " (\"'\", 'this'): True,\n",
       " ('this', \"isn't\"): True,\n",
       " (\"isn't\", 'AI'): True,\n",
       " ('AI', \"'\"): True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TwSentiment import bag_of_words_and_bigrams\n",
    "\n",
    "# this will return a dictionary of features,\n",
    "# we just list the features present in this tweet\n",
    "bag_of_words_and_bigrams(tokenizer.tokenize(tweet_df.text.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the logistic regression classifier\n",
    "\n",
    "https://www.dropbox.com/s/09rw6a85f7ezk31/sklearn_SGDLogReg_.pickle.zip?dl=1\n",
    "\n",
    "I trained this classifier on this dataset: http://help.sentiment140.com/for-students/, following the approach from this paper: http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\n",
    "\n",
    "This is a set of 14 million tweets with emoticons. Tweets containing \"sad\" emoticons (7 million) are considered negative and tweets with \"happy\" emoticons (7 million) are considered positive.\n",
    "\n",
    "I used a Logistic Regression classifier with L2 regularization that I optimized with a 10 fold cross-validation using $F_1$ score as a metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator DictVectorizer from version pre-0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator SGDClassifier from version pre-0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator Pipeline from version pre-0.18 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# the classifier is saved in a \"pickle\" file\n",
    "import pickle\n",
    "\n",
    "with open('sklearn_SGDLogReg_.pickle', 'rb') as fopen:\n",
    "    classifier_dict = pickle.load(fopen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_inv_mapper': {0: 'neg', 1: 'pos'},\n",
       " 'label_mapper': {'neg': 0, 'pos': 1},\n",
       " 'sklearn_pipeline': Pipeline(steps=[('feat_vectorizer', DictVectorizer(dtype=<class 'numpy.int8'>, separator='=', sort=False,\n",
       "         sparse=True)), ('classifier', SGDClassifier(alpha=7.847599703514622e-06, average=False, class_weight=None,\n",
       "        epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "        learning_rate='optimal', loss='log', n_iter=10, n_jobs=1,\n",
       "        penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "        warm_start=False))])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier_dict contain the classifier and label mappers\n",
    "# that I added so that we remember how the classes are \n",
    "# encoded\n",
    "classifier_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is in fact contained in a pipeline.\n",
    "A sklearn pipeline allows to assemble several transformation of your data (http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipline = classifier_dict['sklearn_pipeline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we have two steps: \n",
    "\n",
    "- Vectorize the textual features (using http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)\n",
    "- Classify the vectorized features (using http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feat_vectorizer',\n",
       "  DictVectorizer(dtype=<class 'numpy.int8'>, separator='=', sort=False,\n",
       "          sparse=True)),\n",
       " ('classifier',\n",
       "  SGDClassifier(alpha=7.847599703514622e-06, average=False, class_weight=None,\n",
       "         epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "         learning_rate='optimal', loss='log', n_iter=10, n_jobs=1,\n",
       "         penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "         warm_start=False))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this the step that will transform a list of textual features to a vector of zeros and ones\n",
    "dict_vect = pipline.steps[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':}',\n",
       " ('so', 'glad'),\n",
       " 'was',\n",
       " ('that', 'was'),\n",
       " ('to', 'make'),\n",
       " 'that',\n",
       " ('skool', 'this'),\n",
       " 'for',\n",
       " 'to',\n",
       " ('hafta', 'get'),\n",
       " 'but',\n",
       " ('!', 'but'),\n",
       " 'get',\n",
       " ('a', 'summer'),\n",
       " ('gay', '!'),\n",
       " (\"don't\", 'have'),\n",
       " ('paper', '!'),\n",
       " 'this',\n",
       " 'so',\n",
       " ('summer', 'skool'),\n",
       " ('!', 'I'),\n",
       " 'paper',\n",
       " ('was', 'gay'),\n",
       " 'make',\n",
       " ('wow', 'that'),\n",
       " ('!', ':}'),\n",
       " 'glad',\n",
       " ('need', 'to'),\n",
       " ('make', 'that'),\n",
       " 'have',\n",
       " ('job', 'for'),\n",
       " ('I', \"don't\"),\n",
       " ('!', 'wow'),\n",
       " 'I',\n",
       " ('this', 'summer'),\n",
       " 'a',\n",
       " ('have', 'summer'),\n",
       " 'gay',\n",
       " ('get', 'a'),\n",
       " 'wow',\n",
       " ('glad', 'I'),\n",
       " 'summer',\n",
       " ('summer', '!'),\n",
       " ('summer', 'job'),\n",
       " 'job',\n",
       " 'skool',\n",
       " ('but', 'I'),\n",
       " ('sure', '!'),\n",
       " 'sure',\n",
       " ('I', 'hafta'),\n",
       " 'need',\n",
       " '!',\n",
       " \"don't\",\n",
       " 'hafta',\n",
       " ('for', 'sure'),\n",
       " ('I', 'need'),\n",
       " ('that', 'paper'),\n",
       " ('only', 'on'),\n",
       " ('hot', 'milo'),\n",
       " 'before',\n",
       " ('5', 'minutes'),\n",
       " 'about',\n",
       " 'the',\n",
       " ',',\n",
       " 'heading',\n",
       " ('yup', ','),\n",
       " ('to', 'the'),\n",
       " 'nice',\n",
       " ('couch', 'for'),\n",
       " ('nice', 'hot'),\n",
       " ('the', 'couch'),\n",
       " ('minutes', 'and'),\n",
       " ('for', 'about'),\n",
       " 'minutes',\n",
       " 'yup',\n",
       " ('@USER', 'yup'),\n",
       " 'on',\n",
       " 'hot',\n",
       " 'then',\n",
       " 'couch',\n",
       " 'and',\n",
       " (',', 'only'),\n",
       " ('before', 'a'),\n",
       " ('for', 'a'),\n",
       " 'only',\n",
       " ('then', \"i'm\"),\n",
       " '@USER',\n",
       " (\"i'm\", 'heading'),\n",
       " ('a', 'nice'),\n",
       " \"i'm\",\n",
       " ('early', 'night'),\n",
       " 'night',\n",
       " '5',\n",
       " ('nice', 'early'),\n",
       " 'early',\n",
       " ('about', '5'),\n",
       " 'milo',\n",
       " ('milo', 'before'),\n",
       " ('and', 'then'),\n",
       " ('heading', 'to'),\n",
       " ('on', 'for'),\n",
       " ('@USER', 'drinks'),\n",
       " ('with', 'mates'),\n",
       " 'promise',\n",
       " 'meet',\n",
       " ('a', 'long'),\n",
       " 'up',\n",
       " ('&', 'a'),\n",
       " 'in',\n",
       " '&',\n",
       " ('long', 'promise'),\n",
       " 'with',\n",
       " ('in', 'london'),\n",
       " ('drinks', 'with'),\n",
       " 'mates',\n",
       " 'drinks',\n",
       " ('london', '&'),\n",
       " ('up', 'with'),\n",
       " ('mates', 'in'),\n",
       " ('meet', 'up'),\n",
       " ('with', '@USER'),\n",
       " ('promise', 'meet'),\n",
       " 'london',\n",
       " 'long',\n",
       " 'out',\n",
       " 'gotta',\n",
       " '...',\n",
       " ('on', \"jason's\"),\n",
       " ('NYC', '!'),\n",
       " ('gotta', 'love'),\n",
       " 'NYC',\n",
       " 'rooftop',\n",
       " ('laying', 'out'),\n",
       " 'laying',\n",
       " (\"jason's\", 'rooftop'),\n",
       " ('rooftop', '...'),\n",
       " 'love',\n",
       " \"jason's\",\n",
       " ('love', 'NYC'),\n",
       " ('out', 'on'),\n",
       " ('...', 'gotta'),\n",
       " ('/', 'better'),\n",
       " 'better',\n",
       " 'online',\n",
       " '/',\n",
       " ('shopping', 'I'),\n",
       " ('easier', '/'),\n",
       " ('better', 'than'),\n",
       " '.',\n",
       " 'irl',\n",
       " ('than', 'shopping'),\n",
       " ('irl', '.'),\n",
       " ('love', 'shopping'),\n",
       " ('-', 'so'),\n",
       " 'much',\n",
       " ('so', 'much'),\n",
       " ('I', 'love'),\n",
       " 'holiday',\n",
       " '-',\n",
       " ('holiday', 'shopping'),\n",
       " ('online', '-'),\n",
       " 'than',\n",
       " ('shopping', 'irl'),\n",
       " 'easier',\n",
       " ('shopping', 'online'),\n",
       " ('much', 'easier'),\n",
       " 'shopping',\n",
       " 'lie',\n",
       " ('off', '/'),\n",
       " ('!', 'tomorrows'),\n",
       " ('golf', 'today'),\n",
       " ('it', '!'),\n",
       " ('lie', 'in'),\n",
       " 'tomorrows',\n",
       " ('-', 'what'),\n",
       " 'golf',\n",
       " 'it',\n",
       " ('what', 'a'),\n",
       " ('..', \"i'm\"),\n",
       " ('/', 'lie'),\n",
       " 'off',\n",
       " 'lovely',\n",
       " ('running', 'and'),\n",
       " ('in', 'for'),\n",
       " 'first',\n",
       " 'running',\n",
       " ('!', '!'),\n",
       " ('a', 'lovely'),\n",
       " 'ages',\n",
       " 'what',\n",
       " (\"i'm\", 'gonna'),\n",
       " ('day', '!'),\n",
       " ('enjoy', 'it'),\n",
       " ('tomorrows', 'my'),\n",
       " ('day', 'off'),\n",
       " ('for', 'ages'),\n",
       " ('ages', '..'),\n",
       " 'day',\n",
       " ('first', 'day'),\n",
       " 'gonna',\n",
       " ('today', '-'),\n",
       " ('lovely', 'day'),\n",
       " ('my', 'first'),\n",
       " 'today',\n",
       " ('gonna', 'enjoy'),\n",
       " 'my',\n",
       " 'enjoy',\n",
       " ('and', 'golf'),\n",
       " '..',\n",
       " ('hope', 'to'),\n",
       " ('business', 'someday'),\n",
       " 'sounds',\n",
       " ('do', 'business'),\n",
       " ('good', ','),\n",
       " 'hope',\n",
       " 'good',\n",
       " ('@USER', 'sounds'),\n",
       " 'do',\n",
       " (',', 'hope'),\n",
       " 'someday',\n",
       " ('to', 'do'),\n",
       " ('sounds', 'good'),\n",
       " 'business',\n",
       " ('LOL', '...'),\n",
       " 'course',\n",
       " 'LOL',\n",
       " (\"it's\", 'hotter'),\n",
       " (\"it's\", 'out'),\n",
       " ('out', '!'),\n",
       " 'XD',\n",
       " ('-', 'LOL'),\n",
       " ('than', 'yesterday'),\n",
       " 'of',\n",
       " ('XD', '&'),\n",
       " ('!', 'XD'),\n",
       " ('of', 'course'),\n",
       " 'yesterday',\n",
       " ('@USER', '-'),\n",
       " ('&', \"it's\"),\n",
       " ('course', \"it's\"),\n",
       " \"it's\",\n",
       " ('hotter', 'than'),\n",
       " ('...', 'of'),\n",
       " 'hotter',\n",
       " \"isn't\",\n",
       " '?',\n",
       " 'bitch',\n",
       " ('it', '?'),\n",
       " ('bitch', \"isn't\"),\n",
       " (\"payback's\", 'a'),\n",
       " ('a', 'bitch'),\n",
       " (\"isn't\", 'it'),\n",
       " \"payback's\",\n",
       " 'by',\n",
       " 'we',\n",
       " 'cows',\n",
       " ('just', 'passed'),\n",
       " ('some', 'dumb'),\n",
       " 'just',\n",
       " ('@USER', 'we'),\n",
       " ('cows', '!'),\n",
       " ('we', 'just'),\n",
       " 'dumb',\n",
       " ('passed', 'by'),\n",
       " ('dumb', 'cows'),\n",
       " 'some',\n",
       " 'passed',\n",
       " ('by', 'some'),\n",
       " 'dinner',\n",
       " 'besties',\n",
       " 'malta',\n",
       " ('the', 'besties'),\n",
       " ('should', 'be'),\n",
       " ('off', 'to'),\n",
       " ('for', 'dinner'),\n",
       " ('the', 'malta'),\n",
       " ('with', 'the'),\n",
       " 'should',\n",
       " ('malta', 'for'),\n",
       " ('good', '!'),\n",
       " ('besties', 'should'),\n",
       " 'be',\n",
       " ('be', 'good'),\n",
       " ('dinner', 'with'),\n",
       " ('freshman', 'on'),\n",
       " 'freshman',\n",
       " (\"i'm\", 'a'),\n",
       " ('on', 'twitter'),\n",
       " ('twitter', '.'),\n",
       " 'twitter',\n",
       " ('a', 'freshman'),\n",
       " 'well.im',\n",
       " 'graduation',\n",
       " 'haha',\n",
       " 'someones',\n",
       " 'lets',\n",
       " 'well',\n",
       " 'saturday',\n",
       " ('do.hows', 'everyones'),\n",
       " ('someones', 'graduation'),\n",
       " 'lol.something',\n",
       " ('graduation', 'part'),\n",
       " ('well', 'well'),\n",
       " ('tonight', 'lol.something'),\n",
       " ('lol.something', 'to'),\n",
       " ('part', 'tonight'),\n",
       " 'do.hows',\n",
       " ('well.im', 'crashing'),\n",
       " ('talk', 'haha'),\n",
       " 'talk',\n",
       " ('crashing', 'someones'),\n",
       " 'everyones',\n",
       " 'tonight',\n",
       " ('saturday', '?'),\n",
       " ('well', 'well.im'),\n",
       " ('to', 'do.hows'),\n",
       " ('lets', 'talk'),\n",
       " 'crashing',\n",
       " 'part',\n",
       " ('?', 'lets'),\n",
       " ('everyones', 'saturday'),\n",
       " 'your',\n",
       " ('the', 'father'),\n",
       " ('.', 'quite'),\n",
       " 'has',\n",
       " 'father',\n",
       " ('much', 'more'),\n",
       " 'were',\n",
       " ('honestly', ','),\n",
       " ('more', 'than'),\n",
       " 'more',\n",
       " 'you',\n",
       " ('has', 'even'),\n",
       " 'amazing',\n",
       " ('amazing', '.'),\n",
       " ('your', 'friend'),\n",
       " (',', 'you'),\n",
       " ('quite', 'honestly'),\n",
       " ('you', 'and'),\n",
       " 'attempted',\n",
       " 'friend',\n",
       " ('do', '.'),\n",
       " 'quite',\n",
       " ('and', 'your'),\n",
       " 'did',\n",
       " ('what', 'the'),\n",
       " ('did', 'so'),\n",
       " ('than', 'what'),\n",
       " ('father', 'has'),\n",
       " ('even', 'attempted'),\n",
       " ('friend', 'did'),\n",
       " 'even',\n",
       " ('@USER', 'you'),\n",
       " ('you', 'were'),\n",
       " ('attempted', 'to'),\n",
       " 'honestly',\n",
       " ('were', 'amazing'),\n",
       " ('is', 'my'),\n",
       " ('follow', '@USER'),\n",
       " ('dinner', 'and'),\n",
       " ('my', 'cupcake'),\n",
       " ('great', 'dinner'),\n",
       " ('@USER', '!'),\n",
       " 'great',\n",
       " 'cupcake',\n",
       " 'follow',\n",
       " ('she', 'is'),\n",
       " ('great', 'friends'),\n",
       " ('!', 'follow'),\n",
       " ('!', 'she'),\n",
       " 'is',\n",
       " ('friends', '!'),\n",
       " ('and', 'great'),\n",
       " 'she',\n",
       " 'friends',\n",
       " ('...', 'and'),\n",
       " ('forget', 'you'),\n",
       " ('@USER', 'hope'),\n",
       " 'how',\n",
       " 'anyone',\n",
       " ('?', '?'),\n",
       " ('soon', '...'),\n",
       " 'pic',\n",
       " ('hope', 'your'),\n",
       " 'soon',\n",
       " 'forget',\n",
       " ('how', 'can'),\n",
       " ('your', 'pic'),\n",
       " 'comes',\n",
       " ('pic', 'comes'),\n",
       " ('can', 'anyone'),\n",
       " ('and', 'how'),\n",
       " ('comes', 'back'),\n",
       " ('anyone', 'forget'),\n",
       " 'back',\n",
       " ('you', '?'),\n",
       " 'can',\n",
       " ('back', 'soon'),\n",
       " ('lovely', '!'),\n",
       " ('hi', 'lovely'),\n",
       " ('@USER', 'hi'),\n",
       " 'hi',\n",
       " ('not', 'even'),\n",
       " ('think', 'new'),\n",
       " 'new',\n",
       " ('!', 'lol'),\n",
       " 'not',\n",
       " ('a', 'car'),\n",
       " 'think',\n",
       " ('I', 'did'),\n",
       " ('owned', 'a'),\n",
       " ('new', 'yorkers'),\n",
       " ('did', 'not'),\n",
       " ('@USER', 'I'),\n",
       " 'car',\n",
       " 'yorkers',\n",
       " 'owned',\n",
       " 'lol',\n",
       " ('even', 'think'),\n",
       " ('yorkers', 'owned'),\n",
       " ('car', '!'),\n",
       " 'knowing',\n",
       " ('bar', 'I'),\n",
       " ('15', '+'),\n",
       " ('at', 'a'),\n",
       " ('people', 'at'),\n",
       " 'randomly',\n",
       " ('randomly', 'showed'),\n",
       " ('love', 'that'),\n",
       " 'also',\n",
       " ('showed', 'up'),\n",
       " ('I', 'also'),\n",
       " ('knowing', '15'),\n",
       " '+',\n",
       " ('that', 'coll'),\n",
       " 'at',\n",
       " 'bar',\n",
       " ('a', 'bar'),\n",
       " 'coll',\n",
       " ('+', 'people'),\n",
       " '15',\n",
       " ('also', 'love'),\n",
       " ('just', 'randomly'),\n",
       " ('up', '!'),\n",
       " 'showed',\n",
       " ('coll', 'just'),\n",
       " ('love', 'knowing'),\n",
       " 'people',\n",
       " ('was', 'a'),\n",
       " ('lol', 'that'),\n",
       " 'movie',\n",
       " ('@USER', 'lol'),\n",
       " ('great', 'movie'),\n",
       " ('a', 'great'),\n",
       " 'or',\n",
       " ('sexier', 'with'),\n",
       " ('thingy', '...'),\n",
       " ('me', 'or'),\n",
       " ('vick', 'even'),\n",
       " 'house',\n",
       " ('him', '!'),\n",
       " ('michael', 'vick'),\n",
       " ('with', 'his'),\n",
       " 'luv',\n",
       " 'thingy',\n",
       " 'vick',\n",
       " 'sexier',\n",
       " 'iz',\n",
       " ('even', 'sexier'),\n",
       " ('iz', 'it'),\n",
       " ('house', 'arrest'),\n",
       " ('it', 'me'),\n",
       " 'ooh',\n",
       " ('arrest', 'thingy'),\n",
       " 'his',\n",
       " 'arrest',\n",
       " 'him',\n",
       " 'michael',\n",
       " ('his', 'house'),\n",
       " ('ooh', 'I'),\n",
       " ('luv', 'him'),\n",
       " ('...', 'ooh'),\n",
       " ('or', 'iz'),\n",
       " 'me',\n",
       " ('iz', 'michael'),\n",
       " ('I', 'luv'),\n",
       " ('sure', 'what'),\n",
       " ('it', 'is'),\n",
       " 'know',\n",
       " ('is', \"it's\"),\n",
       " ('-', 'not'),\n",
       " ('all', 'I'),\n",
       " ('is', '..'),\n",
       " ('..', 'all'),\n",
       " ('I', 'know'),\n",
       " 'all',\n",
       " 'funny',\n",
       " (\"it's\", 'funny'),\n",
       " ('what', 'it'),\n",
       " ('know', 'is'),\n",
       " ('not', 'sure'),\n",
       " 'always',\n",
       " ('the', 'world'),\n",
       " 'talent',\n",
       " 'amazed',\n",
       " ('at', 'the'),\n",
       " ('always', 'amazed'),\n",
       " ('talent', 'in'),\n",
       " ('amount', 'of'),\n",
       " ('world', '.'),\n",
       " ('the', 'amount'),\n",
       " ('of', 'musical'),\n",
       " ('musical', 'talent'),\n",
       " ('support', 'your'),\n",
       " ('local', 'musician'),\n",
       " ('your', 'local'),\n",
       " ('in', 'the'),\n",
       " ('musician', '!'),\n",
       " 'amount',\n",
       " 'local',\n",
       " 'world',\n",
       " 'support',\n",
       " ('amazed', 'at'),\n",
       " ('.', 'support'),\n",
       " 'musician',\n",
       " 'musical',\n",
       " 'like',\n",
       " 'read',\n",
       " ('I', 'should'),\n",
       " ('like', 'a'),\n",
       " ('book', 'I'),\n",
       " ('sounds', 'like'),\n",
       " 'book',\n",
       " ('a', 'book'),\n",
       " ('should', 'read'),\n",
       " ('falls', 'away'),\n",
       " ('listening', 'to'),\n",
       " 'favs',\n",
       " ('*', 'one'),\n",
       " ('my', 'favs'),\n",
       " 'listening',\n",
       " ('favs', '@USER'),\n",
       " 'away',\n",
       " ('to', '*'),\n",
       " '*',\n",
       " 'falls',\n",
       " ('away', '*'),\n",
       " ('one', 'of'),\n",
       " ('of', 'my'),\n",
       " 'one',\n",
       " ('*', 'falls'),\n",
       " ('loves', 'youtube'),\n",
       " 'youtube',\n",
       " 'loves',\n",
       " 'luck',\n",
       " ('@USER', 'good'),\n",
       " ('good', 'luck'),\n",
       " 'andy',\n",
       " ('luck', 'andy'),\n",
       " ('@USER', 'on'),\n",
       " ('with', 'it'),\n",
       " (',', \"i'm\"),\n",
       " (',', 'on'),\n",
       " ('!', 'and'),\n",
       " 'controversy',\n",
       " ('on', 'the'),\n",
       " 'sitting',\n",
       " ('my', 'tweet'),\n",
       " ('tweet', ','),\n",
       " ('welcome', '!'),\n",
       " ('it', 'then'),\n",
       " ('your', 'welcome'),\n",
       " ('sitting', 'on'),\n",
       " ('then', ','),\n",
       " ('edge', 'of'),\n",
       " ('controversy', '!'),\n",
       " 'edge',\n",
       " 'tweet',\n",
       " 'welcome',\n",
       " (\"i'm\", 'sitting'),\n",
       " ('the', 'controversy'),\n",
       " ('the', 'edge'),\n",
       " ('on', 'with'),\n",
       " ('alright', '@USER'),\n",
       " ('@USER', '.'),\n",
       " 'alright',\n",
       " ('do', 'this'),\n",
       " ('.', \"let's\"),\n",
       " \"let's\",\n",
       " (\"let's\", 'do'),\n",
       " ('enjoy', 'your'),\n",
       " ('@USER', 'awww'),\n",
       " ('awww', 'well'),\n",
       " ('well', 'enjoy'),\n",
       " ('your', \"'\"),\n",
       " (\"'\", 'tini'),\n",
       " 'awww',\n",
       " 'tini',\n",
       " \"'\",\n",
       " ('bak', 'from'),\n",
       " ('from', 'construction'),\n",
       " 'bak',\n",
       " 'from',\n",
       " 'construction',\n",
       " 'doll',\n",
       " ('URL', ','),\n",
       " 'URL',\n",
       " ('doll', 'URL'),\n",
       " (',', 'URL'),\n",
       " ('what', 'do'),\n",
       " ('think', '?'),\n",
       " ('do', 'you'),\n",
       " ('you', 'think'),\n",
       " 'bed',\n",
       " 'im',\n",
       " 'bella',\n",
       " (',', 'what'),\n",
       " ('im', 'off'),\n",
       " ('?', 'im'),\n",
       " ('bed', 'now'),\n",
       " 'now',\n",
       " ('to', 'bed'),\n",
       " ('bella', 'doll'),\n",
       " ('@USER', 'all'),\n",
       " ('all', 'about'),\n",
       " ('the', 'trust'),\n",
       " 'trust',\n",
       " ('about', 'the'),\n",
       " ('a', 'softy'),\n",
       " ('I', 'feel'),\n",
       " ('softy', 'now'),\n",
       " 'feel',\n",
       " ('@USER', 'LOL'),\n",
       " 'thanks',\n",
       " ('LOL', 'thanks'),\n",
       " ('...', 'I'),\n",
       " ('thanks', '...'),\n",
       " 'softy',\n",
       " ('feel', 'like'),\n",
       " ('I', 'appreciate'),\n",
       " ('am', 'glad'),\n",
       " 'kind',\n",
       " ('resonate', 'with'),\n",
       " ('the', 'words'),\n",
       " 'appreciate',\n",
       " ('words', 'resonate'),\n",
       " 'resonate',\n",
       " 'am',\n",
       " ('your', 'kind'),\n",
       " ('that', 'the'),\n",
       " ('words', '.'),\n",
       " ('glad', 'that'),\n",
       " ('kind', 'words'),\n",
       " ('appreciate', 'your'),\n",
       " ('I', 'am'),\n",
       " ('.', 'I'),\n",
       " 'words',\n",
       " ('with', 'you'),\n",
       " ('good', 'night'),\n",
       " ('sleeping', 'now'),\n",
       " ('now', '!'),\n",
       " ('will', 'be'),\n",
       " 'sleeping',\n",
       " ('be', 'sleeping'),\n",
       " 'everyone',\n",
       " ('!', 'good'),\n",
       " ('everyone', '!'),\n",
       " 'will',\n",
       " ('night', 'everyone'),\n",
       " ('the', 'girl'),\n",
       " 'mom',\n",
       " ('I', 'want'),\n",
       " ('s', 'mom'),\n",
       " ('time', '..'),\n",
       " ('please', '?'),\n",
       " 'please',\n",
       " ('..', 'and'),\n",
       " ('answer', 'me'),\n",
       " 'want',\n",
       " ('carly', '�'),\n",
       " 'answer',\n",
       " ('to', 'see'),\n",
       " ('and', 'sam'),\n",
       " 'any',\n",
       " 'time',\n",
       " ('?', 'can'),\n",
       " ('?', 'xoxo'),\n",
       " ('who', 'is'),\n",
       " ('@USER', ':'),\n",
       " 'sam',\n",
       " ('sam', '�'),\n",
       " ('girl', '?'),\n",
       " '�',\n",
       " ('me', 'please'),\n",
       " ('see', 'carly'),\n",
       " ('mom', 'at'),\n",
       " ('any', 'time'),\n",
       " ('is', 'the'),\n",
       " 'see',\n",
       " ('s', 'who'),\n",
       " (':', 'I'),\n",
       " ('you', 'answer'),\n",
       " ('�', 's'),\n",
       " ':',\n",
       " 'girl',\n",
       " ('URL', '?'),\n",
       " ('can', 'you'),\n",
       " 'who',\n",
       " 'carly',\n",
       " ('at', 'any'),\n",
       " 'xoxo',\n",
       " 's',\n",
       " ('xoxo', 'URL'),\n",
       " ('want', 'to'),\n",
       " 'actually',\n",
       " 'cousin',\n",
       " ('@USER', 'was'),\n",
       " ('my', 'cousin'),\n",
       " 'done',\n",
       " ('actually', 'done'),\n",
       " ('was', 'actually'),\n",
       " ('done', 'by'),\n",
       " ('by', 'my'),\n",
       " ('A', 'new'),\n",
       " ('there', 'was'),\n",
       " ('comments', ','),\n",
       " 'comments',\n",
       " ('!', 'very'),\n",
       " ('yanks', '&'),\n",
       " 'A',\n",
       " ('harsh', 'banter'),\n",
       " ('check', 'the'),\n",
       " ('video', 'from'),\n",
       " 'yanks',\n",
       " ('&', 'brits'),\n",
       " 'there',\n",
       " ('very', 'stupid'),\n",
       " ('the', 'comments'),\n",
       " 'between',\n",
       " ('and', 'check'),\n",
       " ('kasabian', '.'),\n",
       " 'check',\n",
       " 'kasabian',\n",
       " 'video',\n",
       " ('from', 'kasabian'),\n",
       " ('brits', '!'),\n",
       " 'very',\n",
       " ('between', 'yanks'),\n",
       " ('new', 'video'),\n",
       " 'brits',\n",
       " ('a', 'harsh'),\n",
       " ('stupid', '!'),\n",
       " (',', 'there'),\n",
       " ('banter', 'between'),\n",
       " 'harsh',\n",
       " 'banter',\n",
       " 'stupid',\n",
       " ('!', 'URL'),\n",
       " ('.', 'and'),\n",
       " 'fine',\n",
       " 'fits',\n",
       " ('you', 'just'),\n",
       " 'black',\n",
       " (\"i'm\", 'sure'),\n",
       " ('black', 'fits'),\n",
       " ('@USER', \"i'm\"),\n",
       " ('sure', 'black'),\n",
       " ('fits', 'you'),\n",
       " ('just', 'fine'),\n",
       " 'mee',\n",
       " ('for', 'mee'),\n",
       " 'vote',\n",
       " ('mee', '.'),\n",
       " ('.', 'URL'),\n",
       " 'xxxx',\n",
       " ('URL', 'xxxx'),\n",
       " ('vote', 'for'),\n",
       " 'second',\n",
       " ('the', 'second'),\n",
       " ('@USER', 'the'),\n",
       " ('one', '.'),\n",
       " ('second', 'one'),\n",
       " ('smile', 'again'),\n",
       " (\"i'm\", 'wearing'),\n",
       " ('playing', 'up'),\n",
       " ('up', 'at'),\n",
       " 'wearing',\n",
       " ('now', 'have'),\n",
       " ('wearing', 'my'),\n",
       " ('been', 'playing'),\n",
       " ('.', '@USER'),\n",
       " 'again',\n",
       " ('so', 'life'),\n",
       " ('my', 'smile'),\n",
       " ('as', 'per'),\n",
       " 'life',\n",
       " 'brighter',\n",
       " 'work',\n",
       " ('much', 'brighter'),\n",
       " 'usual',\n",
       " ('brighter', '.'),\n",
       " 'as',\n",
       " ('again', 'now'),\n",
       " 'playing',\n",
       " ('usual', 'so'),\n",
       " ('at', 'work'),\n",
       " ('life', 'is'),\n",
       " 'smile',\n",
       " ('have', 'been'),\n",
       " 'per',\n",
       " ('is', 'much'),\n",
       " ('per', 'usual'),\n",
       " 'been',\n",
       " ('work', 'as'),\n",
       " 'giveaway',\n",
       " 'i',\n",
       " (',', 'i'),\n",
       " ('much', 'for'),\n",
       " ('!', 'come'),\n",
       " ('entering', 'my'),\n",
       " ('thanks', 'so'),\n",
       " 'entering',\n",
       " ('back', 'tomorrow'),\n",
       " ('lily', 'giveaway'),\n",
       " ('giveaway', '!'),\n",
       " ('pearl', 'necklace'),\n",
       " ('&', 'lily'),\n",
       " 'necklace',\n",
       " ('tomorrow', ','),\n",
       " ('come', 'back'),\n",
       " 'come',\n",
       " ('have', 'a'),\n",
       " ('i', 'have'),\n",
       " ('for', 'entering'),\n",
       " 'scheduled',\n",
       " 'tomorrow',\n",
       " ('jack', '&'),\n",
       " ('my', 'jack'),\n",
       " ('giveaway', 'scheduled'),\n",
       " ('@USER', 'thanks'),\n",
       " 'lily',\n",
       " 'pearl',\n",
       " ('a', 'pearl'),\n",
       " ('necklace', 'giveaway'),\n",
       " 'jack',\n",
       " ('sounds', 'fun'),\n",
       " ('wish', 'i'),\n",
       " 'fun',\n",
       " (',', 'wish'),\n",
       " ('fun', ','),\n",
       " ('have', 'gone'),\n",
       " 'could',\n",
       " ('could', 'have'),\n",
       " 'wish',\n",
       " 'gone',\n",
       " ('gone', '!'),\n",
       " ('i', 'could'),\n",
       " ('I', 'guessed'),\n",
       " 'guessed',\n",
       " 'right',\n",
       " ('guessed', 'right'),\n",
       " 'prize',\n",
       " ('YAY', '!'),\n",
       " ('prize', '?'),\n",
       " ('I', 'get'),\n",
       " ('do', 'I'),\n",
       " ('a', 'prize'),\n",
       " ('right', '...'),\n",
       " 'YAY',\n",
       " ('...', 'do'),\n",
       " ('@USER', 'YAY'),\n",
       " 'interwebz',\n",
       " ('hay', ','),\n",
       " 'gunna',\n",
       " (',', 'goodnight'),\n",
       " ('the', 'hay'),\n",
       " ('interwebz', '!'),\n",
       " 'hit',\n",
       " ('gunna', 'hit'),\n",
       " 'goodnight',\n",
       " ('goodnight', 'interwebz'),\n",
       " 'hay',\n",
       " ('hit', 'the'),\n",
       " 'x',\n",
       " ('on', 'my'),\n",
       " ('!', 'x'),\n",
       " ('-', 'lovely'),\n",
       " ('your', 'comment'),\n",
       " ('my', 'blog'),\n",
       " ('comment', 'on'),\n",
       " ('thanks', 'for'),\n",
       " ('blog', '-'),\n",
       " ('for', 'your'),\n",
       " 'blog',\n",
       " 'comment',\n",
       " ('familyyy', '..'),\n",
       " ('out', 'with'),\n",
       " 'familyyy',\n",
       " 'text',\n",
       " ('exhausteddd', '..'),\n",
       " 'exhausteddd',\n",
       " ('is', 'exhausteddd'),\n",
       " ('..', 'text'),\n",
       " ('with', 'familyyy'),\n",
       " ('..', 'out'),\n",
       " (\"i've\", 'miss'),\n",
       " 'miss',\n",
       " \"you're\",\n",
       " ('miss', 'u'),\n",
       " ('!', \"i've\"),\n",
       " 'hereee',\n",
       " ('!', \"you're\"),\n",
       " \"i've\",\n",
       " 'heey',\n",
       " (\"you're\", 'hereee'),\n",
       " ('heey', '!'),\n",
       " 'u',\n",
       " ('hereee', '!'),\n",
       " ('@USER', 'heey'),\n",
       " ('u', '!'),\n",
       " 'finished',\n",
       " ('finished', 'pass'),\n",
       " 'pass',\n",
       " 'assignment',\n",
       " ('pass', 'assignment'),\n",
       " ('yay', 'finished'),\n",
       " 'yay',\n",
       " ('comes', 'quik'),\n",
       " ('quik', '!'),\n",
       " 'lucky',\n",
       " ('night', 'comes'),\n",
       " ('aaahhh', 'lucky'),\n",
       " ('hopefully', 'tomorrow'),\n",
       " 'aaahhh',\n",
       " ('@USER', 'aaahhh'),\n",
       " ('tomorrow', 'night'),\n",
       " ('!', 'hopefully'),\n",
       " ('lucky', 'u'),\n",
       " 'quik',\n",
       " 'hopefully',\n",
       " ('good', 'isht'),\n",
       " ('.', 'good'),\n",
       " 'isht',\n",
       " ('easy', 'shawty'),\n",
       " ('shawty', '.'),\n",
       " ('be', 'easy'),\n",
       " ('@USER', 'be'),\n",
       " 'shawty',\n",
       " 'easy',\n",
       " 'flowin',\n",
       " 'visitor',\n",
       " ('got', 'a'),\n",
       " ('-', '-'),\n",
       " ('a', 'visitor'),\n",
       " ('-', 'got'),\n",
       " ('flowin', 'today'),\n",
       " 'got',\n",
       " ('visitor', '!'),\n",
       " 'go',\n",
       " ('sneek', 'out'),\n",
       " ('lex', '<3'),\n",
       " ('go', 'with'),\n",
       " \"i'll\",\n",
       " ('bahah', 'lex'),\n",
       " ('you', 'bahah'),\n",
       " 'lex',\n",
       " ('and', 'go'),\n",
       " (\"i'll\", 'sneek'),\n",
       " ('out', 'and'),\n",
       " '<3',\n",
       " 'sneek',\n",
       " 'bahah',\n",
       " ('hanging', 'with'),\n",
       " ('with', 'amber'),\n",
       " ('amber', '.'),\n",
       " 'hanging',\n",
       " 'amber',\n",
       " ('best', 'buy'),\n",
       " ('@USER', 'of'),\n",
       " ('to', 'best'),\n",
       " ('after', 'work'),\n",
       " 'straight',\n",
       " 'best',\n",
       " 'buy',\n",
       " ('!', \"i'll\"),\n",
       " ('be', 'heading'),\n",
       " ('course', '!'),\n",
       " 'after',\n",
       " ('buy', 'straight'),\n",
       " ('straight', 'after'),\n",
       " (\"i'll\", 'be'),\n",
       " ('can', 'move'),\n",
       " ('flight', 'so'),\n",
       " ('please', 'confirm'),\n",
       " ('move', 'on'),\n",
       " ('so', 'i'),\n",
       " 'flight',\n",
       " ('zest', 'air'),\n",
       " 'move',\n",
       " ('confirm', 'my'),\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_vect.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3887274"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of features\n",
    "len(dict_vect.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'all', ',', 'I', 'am', 'very', 'happy', 'today']\n",
      "{'hi': True, 'all': True, ',': True, 'I': True, 'am': True, 'very': True, 'happy': True, 'today': True, ('hi', 'all'): True, ('all', ','): True, (',', 'I'): True, ('I', 'am'): True, ('am', 'very'): True, ('very', 'happy'): True, ('happy', 'today'): True}\n",
      "(1, 3887274)\n"
     ]
    }
   ],
   "source": [
    "# a little example\n",
    "text = 'Hi all, I am very happy today'\n",
    "# first tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# list features\n",
    "features = bag_of_words_and_bigrams(tokens)\n",
    "print(features)\n",
    "\n",
    "# vectorize features\n",
    "X = dict_vect.transform(features)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is a special kind of numpy array. beacause it is extremely sparse\n",
    "# it can be encoded to take less space in memory\n",
    "# if we want to see it fully, we can use .toarray()\n",
    "\n",
    "# number of non-zero values in X:\n",
    "X.toarray().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping between the list of features and the vector of zeros and ones is done when you train the pipeline with its `.fit` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classifing the tweet\n",
    "Now that we have vector representing the presence of features in a tweet, we can apply our logistic regression classifier to compute the probability that a tweet belong to the \"sad\" or \"happy\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = pipline.steps[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=7.847599703514622e-06, average=False, class_weight=None,\n",
       "       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', n_iter=10, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03320511,  0.28887428, -0.29384334, ..., -0.00785218,\n",
       "        -0.00785218, -0.00218135]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the weights of the logistic regression\n",
    "classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3887274)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have as many weights as features\n",
    "classifier.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20885619])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plus the intrecept \n",
    "classifier.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.09908401])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the weight associated with a given feature\n",
    "x = dict_vect.transform({'bad': True})\n",
    "_, ind = np.where(x.todense())\n",
    "classifier.coef_[0,ind]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06994618,  0.93005382]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the probability for a specific tweet\n",
    "classifier.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn pipeline to group the two last steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06994618,  0.93005382]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline.predict_proba(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see to numbers, the first one is the probability of the tweet being sad, the second one is the probability of the tweet being happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that:\n",
    "pipline.predict_proba(features).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together:\n",
    "\n",
    "We will use the class `TweetClassifier` from TwSentiment.py that puts together this process for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TwSentiment import TweetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twClassifier = TweetClassifier(pipline,\n",
    "                              tokenizer=tokenizer,\n",
    "                              feature_extractor=bag_of_words_and_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', array([ 0.06994618,  0.93005382]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "text = 'Hi all, I am very happy today'\n",
    "twClassifier.classify_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['pos', 'neg'], \n",
       "       dtype='<U3'), array([[ 0.14707178,  0.85292822],\n",
       "        [ 0.90656453,  0.09343547]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the classify text method also accepts a list of text as input\n",
    "twClassifier.classify_text(['great day today!', 'bad day today...'])\n",
    "# the classify text method also accepts a list of text as input\n",
    "# twClassifier.classify_text(['not sad', 'not happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now classify our tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_clas, prob = twClassifier.classify_text(tweet_df.text.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the result to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_df['pos_class'] = (emo_clas == 'pos')\n",
    "tweet_df['pos_prob'] = prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>pos_class</th>\n",
       "      <th>pos_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'This Isn't AI'</td>\n",
       "      <td>860217132763754497</td>\n",
       "      <td>211638860</td>\n",
       "      <td>False</td>\n",
       "      <td>0.316765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @IoTRecruiting: How will Cognitive Computin...</td>\n",
       "      <td>860217137058635776</td>\n",
       "      <td>3226000831</td>\n",
       "      <td>True</td>\n",
       "      <td>0.814357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trou* https://t.co/FlQdwMFbmh</td>\n",
       "      <td>860217138908475397</td>\n",
       "      <td>3070524046</td>\n",
       "      <td>True</td>\n",
       "      <td>0.631905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @InvestorIdeas: https://t.co/ZFdyj2RNXV - #...</td>\n",
       "      <td>860217141827518464</td>\n",
       "      <td>837339093214318593</td>\n",
       "      <td>False</td>\n",
       "      <td>0.416576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepStack: Expert-level artificial intelligenc...</td>\n",
       "      <td>860217143756857344</td>\n",
       "      <td>766378262259986437</td>\n",
       "      <td>True</td>\n",
       "      <td>0.818437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            tweet_id  \\\n",
       "0                                    'This Isn't AI'  860217132763754497   \n",
       "1  RT @IoTRecruiting: How will Cognitive Computin...  860217137058635776   \n",
       "2                      Trou* https://t.co/FlQdwMFbmh  860217138908475397   \n",
       "3  RT @InvestorIdeas: https://t.co/ZFdyj2RNXV - #...  860217141827518464   \n",
       "4  DeepStack: Expert-level artificial intelligenc...  860217143756857344   \n",
       "\n",
       "              user_id pos_class  pos_prob  \n",
       "0           211638860     False  0.316765  \n",
       "1          3226000831      True  0.814357  \n",
       "2          3070524046      True  0.631905  \n",
       "3  837339093214318593     False  0.416576  \n",
       "4  766378262259986437      True  0.818437  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0mNM+glqXH/DzEhKCqQHZvAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x222e2e9ab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of probability\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "h = plt.hist(tweet_df.pos_prob, bins=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to classify users based on the class of their tweets.\n",
    "Pandas allows to easily group tweets per users using the [groupy](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) method of DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_group = tweet_df.groupby('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.groupby.DataFrameGroupBy'>\n"
     ]
    }
   ],
   "source": [
    "print(type(user_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>pos_class</th>\n",
       "      <th>pos_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>RT @hackaday: Google AIY: Artificial Intellige...</td>\n",
       "      <td>860217395314647040</td>\n",
       "      <td>787345</td>\n",
       "      <td>True</td>\n",
       "      <td>0.885622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text            tweet_id  \\\n",
       "85  RT @hackaday: Google AIY: Artificial Intellige...  860217395314647040   \n",
       "\n",
       "    user_id pos_class  pos_prob  \n",
       "85   787345      True  0.885622  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at one of the group\n",
    "groups = user_group.groups\n",
    "uid = list(groups.keys())[5]\n",
    "user_group.get_group(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to make a function that takes the dataframe of tweets grouped by users and return the class of the users\n",
    "def get_user_emo(group):\n",
    "    num_pos = group.pos_class.sum()\n",
    "    num_tweets = group.pos_class.size\n",
    "    if num_pos/num_tweets > 0.5:\n",
    "        return 'pos'\n",
    "    elif num_pos/num_tweets < 0.5:\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the function to each group\n",
    "user_df = user_group.apply(get_user_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "785        pos\n",
       "12774      pos\n",
       "63433      pos\n",
       "685063     neg\n",
       "779302     pos\n",
       "787345     pos\n",
       "994761     pos\n",
       "1246421    neg\n",
       "1294621    pos\n",
       "1308181    pos\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a pandas Series where the index are the user_id\n",
    "user_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add this information to the graph we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.read_graphml('twitter_lcc_AI2.graphml', node_type=int)\n",
    "\n",
    "for n in G.nodes_iter():\n",
    "    if n in user_df.index:\n",
    "        # here we look at the value of the user_df series at the position where the index \n",
    "        # is equal to the user_id of the node\n",
    "        G.node[n]['emotion'] = user_df.loc[user_df.index == n].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Kasparov63'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have added an attribute 'emotion' to the nodes\n",
    "G.node[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the graph to open it with Gephi\n",
    "nx.write_graphml(G, 'twitter_lcc_emo_AI2.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
